import streamlit as st
import json
import os
from openai import OpenAI

# ==========================================
# 1. API é…ç½® (ç”¨äºå”¤é†’ AI è®²é¢˜åŠŸèƒ½)
# ==========================================
GITHUB_TOKEN =st.secrets["GITHUB_TOKEN"]

@st.cache_resource
def get_client():
    if GITHUB_TOKEN and "å¡«åœ¨è¿™é‡Œ" not in GITHUB_TOKEN:
        return OpenAI(base_url="https://models.inference.ai.azure.com", api_key=GITHUB_TOKEN)
    return None

# ==========================================
# 2. è¯»å–å¸¦ç­”æ¡ˆçš„ç»ˆæé¢˜åº“
# ==========================================
@st.cache_data
def load_data():
    file_path = "book_complete.json"
    if not os.path.exists(file_path):
        return []
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)
    return [page for page in data if page.get("data")]

# ==========================================
# 3. æ„å»ºç½‘é¡µç•Œé¢
# ==========================================
st.set_page_config(page_title="æ³•è¯­æ™ºèƒ½åˆ·é¢˜å™¨", page_icon="ğŸ‡«ğŸ‡·", layout="centered")
st.title("ğŸ‡«ğŸ‡· æ³•è¯­ç»ˆææ™ºèƒ½ç™½æ¿")
st.caption("åŸºäºå®Œæ•´ç¦»çº¿é¢˜åº“ï¼Œæ”¯æŒç§’é€Ÿå¯¹ç­”æ¡ˆä¸ AI åå¸ˆè®²è§£")

pages = load_data()
if not pages:
    st.error("æ‰¾ä¸åˆ° book_complete.json æ–‡ä»¶ï¼Œè¯·ç¡®ä¿å®ƒå’Œ app.py åœ¨åŒä¸€ä¸ªæ–‡ä»¶å¤¹ï¼")
    st.stop()

# --- ä¾§è¾¹æ å¯¼èˆª ---
st.sidebar.header("ğŸ“– é¢˜åº“å¯¼èˆª")
page_options = {f"ç¬¬ {p['page']} é¡µ (å…± {len(p['data'])} é¢˜)": p for p in pages}
selected_option = st.sidebar.selectbox("é€‰æ‹©ä»Šå¤©è¦åˆ·çš„é¡µé¢", list(page_options.keys()))
selected_page_data = page_options[selected_option]

st.markdown(f"### å½“å‰ç»ƒä¹ ï¼š{selected_option.split(' ')[0]}")
st.divider()

client = get_client()

# --- éå†å¹¶æ˜¾ç¤ºå½“å‰é¡µçš„æ‰€æœ‰é¢˜ç›® ---
for idx, q in enumerate(selected_page_data["data"]):
    block = q.get('exercise_block') or 'ç»ƒä¹ '
    num = q.get('question_number') or (idx + 1)
    st.subheader(f"âœï¸ {block} - é¢˜ {num}")
    
   # é¢˜ç›®å’Œæç¤º (æ”¾å¤§å­—ä½“å‡çº§ç‰ˆ)
    st.markdown(
        f"<div style='font-size: 36px; line-height: 1.6; margin-bottom: 10px;'>"
        f"<b>é¢˜ç›®ï¼š</b> <code>{q['question_text']}</code>"
        f"</div>", 
        unsafe_allow_html=True
    )
    if q.get('hints'):
        # é¡ºä¾¿æŠŠæç¤ºè¯ä¹Ÿç¨å¾®æ”¾å¤§ä¸€ç‚¹
        st.markdown(
            f"<div style='font-size: 18px; color: #026873; background-color: #E0F7FA; padding: 10px; border-radius: 5px; margin-bottom: 15px;'>"
            f"ğŸ’¡ <b>æç¤ºè¯:</b> {q['hints']}"
            f"</div>", 
            unsafe_allow_html=True
        )
        
    # æ¥æ”¶ç”¨æˆ·è¾“å…¥
    user_answer = st.text_input("ğŸ“ ä½ çš„ç­”æ¡ˆï¼š", key=f"input_{selected_page_data['page']}_{idx}")
    standard_answer = q.get('answer', '')
    
    # å°†æŒ‰é’®å¹¶æ’æ”¾åœ¨ä¸€èµ·
    col1, col2 = st.columns([1, 1])
    with col1:
        check_btn = st.button("âœ… å¯¹ç­”æ¡ˆ", key=f"check_{selected_page_data['page']}_{idx}")
    with col2:
        explain_btn = st.button("ğŸ§  è¯· AI è€å¸ˆè®²è§£", key=f"explain_{selected_page_data['page']}_{idx}")
        
    # --- é€»è¾‘ 1ï¼šç§’é€Ÿå¯¹ç­”æ¡ˆ (æœ¬åœ°åˆ¤æ–­) ---
    if check_btn:
        if not user_answer.strip():
            st.warning("ä½ è¿˜æ²¡å†™ç­”æ¡ˆå‘¢ï¼")
        else:
            if not standard_answer:
                st.warning("âš ï¸ è¿™é“é¢˜åœ¨ä¹¦æœ«ç­”æ¡ˆåº“é‡Œæ²¡æœ‰æ‰¾åˆ°ï¼Œè¯·è‡ªè¡Œåˆ¤æ–­æˆ–ç‚¹å‡» AI è®²è§£ã€‚")
            # å¿½ç•¥å¤§å°å†™å’Œå‰åç©ºæ ¼è¿›è¡Œæ¯”å¯¹
            elif user_answer.strip().lower() == standard_answer.strip().lower():
                st.success(f"ğŸ‰ å®Œå…¨æ­£ç¡®ï¼æ ‡å‡†ç­”æ¡ˆå°±æ˜¯ï¼š**{standard_answer}**")
            else:
                st.error(f"âŒ ç­”é”™äº†ã€‚ä½ çš„ç­”æ¡ˆï¼š`{user_answer}` | æ ‡å‡†ç­”æ¡ˆï¼š**`{standard_answer}`**")
                
    # --- é€»è¾‘ 2ï¼šå¬å”¤ AI è€å¸ˆè®²é¢˜ ---
    if explain_btn:
        if not client:
            st.warning("è¯·åœ¨ä»£ç å¼€å¤´å¡«å…¥ä½ çš„ GITHUB_TOKEN æ‰èƒ½å”¤é†’ AI è€å¸ˆå“¦ï¼")
        else:
            with st.spinner("AI è€å¸ˆæ­£åœ¨å¤‡è¯¾ä¸­..."):
                prompt = f"""
                è¿™æ˜¯ä¸€é“æ³•è¯­è¯­æ³•é¢˜ï¼š
                åŸé¢˜: "{q['question_text']}"
                æç¤ºè¯: "{q.get('hints', 'æ— ')}"
                æ ‡å‡†ç­”æ¡ˆ: "{standard_answer}"
                å­¦ç”Ÿçš„ç­”æ¡ˆ: "{user_answer}"
                
                è¯·ä½ æ‰®æ¼”ä¸€åå¹½é»˜ä¸“ä¸šçš„æ³•è¯­è€å¸ˆï¼š
                1. è§£é‡Šä¸ºä»€ä¹ˆæ ‡å‡†ç­”æ¡ˆæ˜¯ "{standard_answer}"ï¼ˆæ¶‰åŠä»€ä¹ˆå…·ä½“çš„æ³•è¯­è¯­æ³•ã€æ—¶æ€æˆ–å˜ä½è§„åˆ™ï¼‰ã€‚
                2. å¦‚æœå­¦ç”Ÿå†™äº†ç­”æ¡ˆä¸”ç­”é”™äº†ï¼Œæ¸©æŸ”åœ°æŒ‡å‡ºä»–ä¸ºä»€ä¹ˆé”™ã€‚
                """
                try:
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": prompt}],
                        temperature=0.3
                    )
                    st.markdown(f"**ğŸ‘¨â€ğŸ« AI è€å¸ˆçš„è§£æï¼š**\n\n{response.choices[0].message.content}")
                except Exception as e:
                    st.error(f"å¬å”¤ AI è€å¸ˆå¤±è´¥: {e}")
                    

    st.divider() # é¢˜ç›®ä¹‹é—´çš„åˆ†å‰²çº¿
